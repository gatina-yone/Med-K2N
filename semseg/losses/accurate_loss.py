#!/usr/bin/env python3
"""
æ›´å‡†ç¡®å’Œé€‚é…çš„æŸå¤±å‡½æ•°å®ç°
ä¸“é—¨é’ˆå¯¹ç—…ç¶ä¿æŒä¼˜åŒ– - å¹²å‡€ç‰ˆæœ¬
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
import os
from torchmetrics.functional import structural_similarity_index_measure as ssim
import torchvision.models as models

class AccurateLoss(nn.Module):
    """æ›´å‡†ç¡®çš„åŒ»å­¦å›¾åƒæŸå¤±å‡½æ•° - ä¸“é—¨é’ˆå¯¹ç—…ç¶ä¿æŒä¼˜åŒ–ï¼Œæ·»åŠ æ„ŸçŸ¥æŸå¤±"""
    
    def __init__(self, device='cuda', lambda_weighted_l1=0.0, lambda_ssim=0.0, 
                 lambda_grad=0.0, lambda_consistency=0.0, lambda_lesion_aware=0.0, 
                 lambda_tv=0.0, lambda_perceptual=0.0,
                 # ä¿æŒå‘åå…¼å®¹çš„æ—§å‚æ•°
                 lambda_l1=0.0):
        super().__init__()
        self.device = device
        
        # åŸºç¡€æŸå¤±å‡½æ•°æƒé‡
        self.lambda_weighted_l1 = lambda_weighted_l1
        self.lambda_ssim = lambda_ssim
        self.lambda_grad = lambda_grad
        self.lambda_consistency = lambda_consistency
        self.lambda_lesion_aware = lambda_lesion_aware
        self.lambda_tv = lambda_tv
        self.lambda_perceptual = lambda_perceptual  # æ–°å¢æ„ŸçŸ¥æŸå¤±
        
        # å‘åå…¼å®¹ï¼šå¦‚æœä½¿ç”¨æ—§å‚æ•°ï¼Œè®¾ç½®ç›¸åº”å±æ€§
        self.lambda_l1 = lambda_l1
        
        # å¦‚æœä½¿ç”¨æ—§å‚æ•°ä¸”æ–°å‚æ•°ä¸º0ï¼Œåˆ™æ˜ å°„
        if lambda_l1 > 0 and lambda_weighted_l1 == 0.0:
            self.lambda_weighted_l1 = lambda_l1
                
        # åˆå§‹åŒ–æŸå¤±å‡½æ•°ç»„ä»¶
        self.gradient_loss = EnhancedGradientLoss()
        if self.lambda_weighted_l1 > 0:
            self.weighted_l1_loss = WeightedL1Loss()
        if self.lambda_lesion_aware > 0:
            self.lesion_aware_loss = LesionAwareLoss()
        
        # åˆå§‹åŒ–æ„ŸçŸ¥æŸå¤± (ä½¿ç”¨VGG19ç‰¹å¾)
        if self.lambda_perceptual > 0:
            self.perceptual_loss = PerceptualLoss(device=device)
    
    def forward(self, pred, target, input_modal=None):
        losses = {}
        total_loss = 0.0
        
        # åŠ æƒL1æŸå¤± - é’ˆå¯¹ç—…ç¶åŒºåŸŸ
        if self.lambda_weighted_l1 > 0:
            if hasattr(self, 'weighted_l1_loss'):
                losses['weighted_l1'] = self.weighted_l1_loss(pred, target) * self.lambda_weighted_l1
            else:
                losses['weighted_l1'] = F.l1_loss(pred, target) * self.lambda_weighted_l1  # å›é€€åˆ°æ™®é€šL1
            total_loss += losses['weighted_l1']
        else:
            losses['weighted_l1'] = torch.tensor(0.0, device=self.device)
        
        # å‘åå…¼å®¹ï¼šä½¿ç”¨å¸¸è§„L1æŸå¤±
        if hasattr(self, 'lambda_l1') and self.lambda_l1 > 0 and self.lambda_weighted_l1 == 0.0:
            l1_loss = F.l1_loss(pred, target)
            losses['l1'] = l1_loss * self.lambda_l1
            total_loss += losses['l1']
        else:
            losses['l1'] = torch.tensor(0.0, device=self.device)
        
        # SSIMæŸå¤± - ä½¿ç”¨ç»Ÿä¸€çš„torchmetricså®ç°
        if self.lambda_ssim > 0:
            # æ­£ç¡®å¤„ç†tanhè¾“å‡ºï¼šå°†[-1,1]æ˜ å°„åˆ°[0,1]åå†è®¡ç®—SSIM
            def normalize_for_ssim(x):
                # æ£€æµ‹è¾“å‡ºèŒƒå›´å¹¶æ­£ç¡®æ˜ å°„
                x_min, x_max = x.min(), x.max()
                if x_min >= -1.01 and x_max <= 1.01 and (x_min < -0.01 or x_max > 1.01):
                    # å¯èƒ½æ˜¯tanhè¾“å‡ºï¼Œæ˜ å°„[-1,1] -> [0,1]
                    return (x + 1.0) * 0.5
                else:
                    # å·²ç»åœ¨[0,1]èŒƒå›´æˆ–éœ€è¦min-maxå½’ä¸€åŒ–
                    return torch.clamp(x, 0.0, 1.0)
            
            pred_norm = normalize_for_ssim(pred)
            target_norm = normalize_for_ssim(target)
            
            ssim_value = ssim(pred_norm, target_norm, data_range=1.0)
            ssim_loss = 1 - ssim_value
            losses['ssim'] = ssim_loss * self.lambda_ssim
            total_loss += losses['ssim']
        else:
            losses['ssim'] = torch.tensor(0.0, device=self.device)
        
        # æ¢¯åº¦æŸå¤±
        if self.lambda_grad > 0:
            losses['gradient'] = self.gradient_loss(pred, target) * self.lambda_grad
            total_loss += losses['gradient']
        else:
            losses['gradient'] = torch.tensor(0.0, device=self.device)
        
        # æ¨¡æ€ä¸€è‡´æ€§æŸå¤±
        if self.lambda_consistency > 0 and input_modal is not None:
            losses['consistency'] = self.calculate_consistency_loss(pred, input_modal) * self.lambda_consistency
            total_loss += losses['consistency']
        else:
            losses['consistency'] = torch.tensor(0.0, device=self.device)
        
        # ç—…ç¶æ„ŸçŸ¥æŸå¤±
        if self.lambda_lesion_aware > 0:
            if hasattr(self, 'lesion_aware_loss'):
                losses['lesion_aware'] = self.lesion_aware_loss(pred, target) * self.lambda_lesion_aware
            else:
                # ç®€å•çš„é«˜å¼ºåº¦åŒºåŸŸL1æŸå¤±ä½œä¸ºå›é€€
                high_intensity_mask = (target > 0.4).float()
                lesion_l1 = torch.abs(pred - target) * high_intensity_mask
                lesion_loss = lesion_l1.sum() / (high_intensity_mask.sum() + 1e-8)
                losses['lesion_aware'] = lesion_loss * self.lambda_lesion_aware
            total_loss += losses['lesion_aware']
        else:
            losses['lesion_aware'] = torch.tensor(0.0, device=self.device)
        
        # TVæŸå¤±
        if self.lambda_tv > 0:
            losses['tv'] = self.calculate_tv_loss(pred) * self.lambda_tv
            total_loss += losses['tv']
        else:
            losses['tv'] = torch.tensor(0.0, device=self.device)
        
        # æ„ŸçŸ¥æŸå¤± (VGGç‰¹å¾)
        if self.lambda_perceptual > 0 and hasattr(self, 'perceptual_loss'):
            losses['perceptual'] = self.perceptual_loss(pred, target) * self.lambda_perceptual
            total_loss += losses['perceptual']
        else:
            losses['perceptual'] = torch.tensor(0.0, device=self.device)
        
        return total_loss, losses
    
    def calculate_consistency_loss(self, pred, input_modal):
        """è®¡ç®—æ¨¡æ€ä¸€è‡´æ€§æŸå¤±"""
        if isinstance(input_modal, list) and len(input_modal) > 0:
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡æ€è®¡ç®—æ¢¯åº¦ç»“æ„ä¸€è‡´æ€§
            input_grad_x = torch.abs(input_modal[0][:, :, :, :-1] - input_modal[0][:, :, :, 1:])
            input_grad_y = torch.abs(input_modal[0][:, :, :-1, :] - input_modal[0][:, :, 1:, :])
            
            pred_grad_x = torch.abs(pred[:, :, :, :-1] - pred[:, :, :, 1:])
            pred_grad_y = torch.abs(pred[:, :, :-1, :] - pred[:, :, 1:, :])
            
            consistency_loss = F.l1_loss(pred_grad_x, input_grad_x) + F.l1_loss(pred_grad_y, input_grad_y)
            return consistency_loss
        return torch.tensor(0.0, device=self.device)
    
    def calculate_tv_loss(self, pred):
        """è®¡ç®—æ€»å˜åˆ†æŸå¤±"""
        batch_size, channels, height, width = pred.shape
        
        h_tv = torch.pow((pred[:, :, 1:, :] - pred[:, :, :height-1, :]), 2).sum()
        w_tv = torch.pow((pred[:, :, :, 1:] - pred[:, :, :, :width-1]), 2).sum()
        
        count_h = batch_size * channels * (height - 1) * width
        count_w = batch_size * channels * height * (width - 1)
        
        return (h_tv / count_h + w_tv / count_w) / 2.0
    



class WeightedL1Loss(nn.Module):
    """åŠ æƒL1æŸå¤± - é’ˆå¯¹ç—…ç¶åŒºåŸŸç»™äºˆæ›´é«˜æƒé‡"""
    
    def __init__(self, focus_threshold=0.3, weight_multiplier=3.0):
        super().__init__()
        self.focus_threshold = focus_threshold
        self.weight_multiplier = weight_multiplier
    
    def forward(self, pred, target):
        # è®¡ç®—å¼ºåº¦æ¢¯åº¦ï¼Œè¯†åˆ«é‡è¦åŒºåŸŸï¼ˆå¦‚ç—…ç¶ï¼‰
        target_grad_x = torch.abs(target[:, :, :, :-1] - target[:, :, :, 1:])
        target_grad_y = torch.abs(target[:, :, :-1, :] - target[:, :, 1:, :])
        
        # å°†æ¢¯åº¦å¡«å……å›åŸå§‹å°ºå¯¸
        target_grad_x_padded = F.pad(target_grad_x, (0, 1, 0, 0), mode='replicate')
        target_grad_y_padded = F.pad(target_grad_y, (0, 0, 0, 1), mode='replicate')
        
        # è®¡ç®—æ¢¯åº¦å¹…å€¼
        gradient_magnitude = torch.sqrt(target_grad_x_padded**2 + target_grad_y_padded**2 + 1e-8)
        
        # å½’ä¸€åŒ–æ¢¯åº¦å¹…å€¼
        grad_min = gradient_magnitude.min()
        grad_max = gradient_magnitude.max()
        if grad_max > grad_min:
            gradient_magnitude = (gradient_magnitude - grad_min) / (grad_max - grad_min + 1e-8)
        
        # åŒæ—¶è€ƒè™‘é«˜å¼ºåº¦åŒºåŸŸï¼ˆç—…ç¶é€šå¸¸æ˜¯é«˜å¼ºåº¦ï¼‰
        high_intensity_mask = (target > self.focus_threshold).float()
        
        # åˆ›å»ºç»¼åˆæƒé‡ï¼šæ¢¯åº¦åŒºåŸŸ + é«˜å¼ºåº¦åŒºåŸŸ
        importance_map = torch.clamp(gradient_magnitude + high_intensity_mask, 0, 1)
        weights = 1.0 + self.weight_multiplier * importance_map
        
        # åŠ æƒL1æŸå¤±
        l1_loss = torch.abs(pred - target)
        weighted_loss = (l1_loss * weights).mean()
        
        return weighted_loss


class LesionAwareLoss(nn.Module):
    """ç—…ç¶æ„ŸçŸ¥æŸå¤± - ä¸“é—¨é’ˆå¯¹é«˜å¼ºåº¦ç—…ç¶åŒºåŸŸ"""
    
    def __init__(self, intensity_threshold=0.4, contrast_threshold=0.2):
        super().__init__()
        self.intensity_threshold = intensity_threshold
        self.contrast_threshold = contrast_threshold
    
    def forward(self, pred, target):
        # è¯†åˆ«é«˜å¼ºåº¦åŒºåŸŸï¼ˆæ½œåœ¨ç—…ç¶ï¼‰
        high_intensity_mask = (target > self.intensity_threshold).float()
        
        # è¯†åˆ«é«˜å¯¹æ¯”åº¦åŒºåŸŸï¼ˆç—…ç¶è¾¹ç¼˜ï¼‰
        # è®¡ç®—å±€éƒ¨æ ‡å‡†å·®æ¥æ£€æµ‹å¯¹æ¯”åº¦
        kernel_size = 5
        padding = kernel_size // 2
        
        # ä½¿ç”¨å·ç§¯è®¡ç®—å±€éƒ¨å‡å€¼
        avg_kernel = torch.ones(1, 1, kernel_size, kernel_size, device=target.device) / (kernel_size * kernel_size)
        if target.shape[1] > 1:
            target_gray = target.mean(dim=1, keepdim=True)
        else:
            target_gray = target
        
        local_mean = F.conv2d(target_gray, avg_kernel, padding=padding)
        local_var = F.conv2d((target_gray - local_mean)**2, avg_kernel, padding=padding)
        local_std = torch.sqrt(local_var + 1e-8)
        
        # æ‰©å±•åˆ°æ‰€æœ‰é€šé“
        if target.shape[1] > 1:
            local_std = local_std.expand_as(target)
        
        contrast_mask = (local_std > self.contrast_threshold).float()
        
        # ç»“åˆé«˜å¼ºåº¦å’Œé«˜å¯¹æ¯”åº¦åŒºåŸŸ
        lesion_mask = torch.clamp(high_intensity_mask + contrast_mask * 0.5, 0, 1)
        
        # ç—…ç¶åŒºåŸŸçš„L1æŸå¤±
        lesion_l1 = torch.abs(pred - target) * lesion_mask
        lesion_loss = lesion_l1.sum() / (lesion_mask.sum() + 1e-8)
        
        # æ•´ä½“L1æŸå¤±ï¼ˆè¾ƒå°æƒé‡ï¼‰
        global_l1 = torch.abs(pred - target).mean()
        
        # ç»“åˆæŸå¤±ï¼šç—…ç¶åŒºåŸŸæƒé‡æ›´é«˜
        return 0.2 * global_l1 + 0.8 * lesion_loss


class EnhancedGradientLoss(nn.Module):
    """å¢å¼ºçš„æ¢¯åº¦æŸå¤±"""
    
    def forward(self, pred, target):
        # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
        if pred.dtype != target.dtype:
            target = target.to(pred.dtype)
        
        # Sobelç®—å­
        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=pred.dtype, device=pred.device).view(1, 1, 3, 3)
        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=pred.dtype, device=pred.device).view(1, 1, 3, 3)
        
        # å¦‚æœæ˜¯å¤šé€šé“ï¼Œè½¬æ¢ä¸ºå•é€šé“
        if pred.shape[1] > 1:
            pred_gray = torch.mean(pred, dim=1, keepdim=True)
            target_gray = torch.mean(target, dim=1, keepdim=True)
        else:
            pred_gray = pred
            target_gray = target
        
        # è®¡ç®—æ¢¯åº¦
        pred_grad_x = F.conv2d(pred_gray, sobel_x, padding=1)
        pred_grad_y = F.conv2d(pred_gray, sobel_y, padding=1)
        target_grad_x = F.conv2d(target_gray, sobel_x, padding=1)
        target_grad_y = F.conv2d(target_gray, sobel_y, padding=1)
        
        # æ¢¯åº¦å¹…å€¼
        pred_grad_mag = torch.sqrt(pred_grad_x**2 + pred_grad_y**2 + 1e-8)
        target_grad_mag = torch.sqrt(target_grad_x**2 + target_grad_y**2 + 1e-8)
        
        # L1æŸå¤±
        gradient_loss = F.l1_loss(pred_grad_mag, target_grad_mag)
        
        return gradient_loss


class AccurateMetrics:
    """ç»Ÿä¸€çš„è¯„ä¼°æŒ‡æ ‡è®¡ç®— - ç¡®ä¿é¡¹ç›®ä¸­SSIMå’ŒPSNRè®¡ç®—çš„ä¸€è‡´æ€§"""
    
    @staticmethod
    def calculate_psnr(pred, target, data_range=1.0):
        """è®¡ç®—PSNR - ç»Ÿä¸€ä½¿ç”¨data_range=1.0"""
        # è½¬æ¢ä¸ºnumpyè¿›è¡Œè®¡ç®—
        pred_np = pred.detach().cpu().numpy()
        target_np = target.detach().cpu().numpy()
        
        # ç¡®ä¿æ•°æ®åœ¨æ­£ç¡®èŒƒå›´å†…
        pred_np = np.clip(pred_np, 0, data_range)
        target_np = np.clip(target_np, 0, data_range)
        
        mse = np.mean((pred_np - target_np) ** 2)
        if mse == 0:
            return 100.0
        return 20 * np.log10(data_range / np.sqrt(mse))
    
    @staticmethod
    def calculate_ssim(pred, target, data_range=1.0):
        """è®¡ç®—SSIM - ä½¿ç”¨ç»Ÿä¸€çš„torchmetricså®ç°ï¼Œdata_range=1.0"""
        # æ­£ç¡®å¤„ç†tanhè¾“å‡ºï¼šå°†[-1,1]æ˜ å°„åˆ°[0,1]
        def normalize_for_ssim(x):
            x_min, x_max = x.min(), x.max()
            if x_min >= -1.01 and x_max <= 1.01 and (x_min < -0.01 or x_max > 1.01):
                # å¯èƒ½æ˜¯tanhè¾“å‡ºï¼Œæ˜ å°„[-1,1] -> [0,1]
                return (x + 1.0) * 0.5
            else:
                # å·²ç»åœ¨[0,1]èŒƒå›´
                return torch.clamp(x, 0.0, data_range)
        
        pred_norm = normalize_for_ssim(pred)
        target_norm = normalize_for_ssim(target)
        
        # ä½¿ç”¨torchmetricsçš„SSIMå®ç°
        ssim_value = ssim(pred_norm, target_norm, data_range=data_range)
        return ssim_value.item() if hasattr(ssim_value, 'item') else float(ssim_value)


class PerceptualLoss(nn.Module):
    """æ„ŸçŸ¥æŸå¤± - ä½¿ç”¨VGG19ç‰¹å¾ï¼Œæ”¹å–„è§†è§‰è´¨é‡"""
    
    def __init__(self, device='cuda', layers=None, vgg_path="/data1/tempf/vgg19-dcbb9e9d.pth"):
        super().__init__()
        self.device = device
        self.vgg_path = vgg_path
        
        # é»˜è®¤ä½¿ç”¨VGG19çš„å¤šä¸ªå±‚
        if layers is None:
            layers = ['relu_1_1', 'relu_2_1', 'relu_3_1', 'relu_4_1', 'relu_5_1']
        self.layers = layers
        
        # åŠ è½½é¢„è®­ç»ƒVGG19 - ä½¿ç”¨è‡ªå®šä¹‰è·¯å¾„
        try:
            if os.path.exists(vgg_path):
                # ä»æœ¬åœ°è·¯å¾„åŠ è½½
                print(f"[PerceptualLoss] ğŸ¯ ä»æœ¬åœ°åŠ è½½VGG19æƒé‡: {vgg_path}")
                vgg = models.vgg19(pretrained=False)
                state_dict = torch.load(vgg_path, map_location='cpu')
                vgg.load_state_dict(state_dict)
                self.vgg = vgg.features.to(device)
            else:
                # å›é€€åˆ°é»˜è®¤é¢„è®­ç»ƒæƒé‡
                print(f"[PerceptualLoss] âš ï¸ æœ¬åœ°VGGæƒé‡æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤é¢„è®­ç»ƒæƒé‡")
                vgg = models.vgg19(pretrained=True).features
                self.vgg = vgg.to(device)
        except Exception as e:
            print(f"[PerceptualLoss] âŒ VGGåŠ è½½å¤±è´¥: {e}")
            print(f"[PerceptualLoss] ğŸ”„ ä½¿ç”¨é»˜è®¤é¢„è®­ç»ƒæƒé‡ä½œä¸ºå¤‡é€‰")
            vgg = models.vgg19(pretrained=True).features  
            self.vgg = vgg.to(device)
        
        # å†»ç»“VGGå‚æ•°
        for param in self.vgg.parameters():
            param.requires_grad = False
        
        # VGGå±‚æ˜ å°„
        self.layer_name_mapping = {
            'relu_1_1': '1',
            'relu_2_1': '6',
            'relu_3_1': '11',
            'relu_4_1': '20',
            'relu_5_1': '29'
        }
        
        # é¢„å¤„ç†ï¼šImageNetæ ‡å‡†åŒ–
        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))
        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))
        
    def forward(self, pred, target):
        # ç¡®ä¿è¾“å…¥åœ¨[0,1]èŒƒå›´
        pred = self.normalize_input(pred)
        target = self.normalize_input(target)
        
        # è½¬æ¢ä¸º3é€šé“ï¼ˆå¦‚æœéœ€è¦ï¼‰
        pred = self.to_3channel(pred)
        target = self.to_3channel(target)
        
        # ç¡®ä¿meanå’Œstdåœ¨ä¸è¾“å…¥ç›¸åŒçš„è®¾å¤‡ä¸Š
        device = pred.device
        mean = self.mean.to(device)
        std = self.std.to(device)
        
        # ImageNetæ ‡å‡†åŒ–
        pred = (pred - mean) / std
        target = (target - mean) / std
        
        # æå–ç‰¹å¾å¹¶è®¡ç®—æŸå¤±
        pred_features = self.extract_features(pred)
        target_features = self.extract_features(target)
        
        loss = 0.0
        weights = [1.0/32, 1.0/16, 1.0/8, 1.0/4, 1.0]  # ä¸åŒå±‚çš„æƒé‡
        
        for i, layer in enumerate(self.layers):
            if layer in pred_features and layer in target_features:
                weight = weights[i] if i < len(weights) else 1.0
                layer_loss = F.mse_loss(pred_features[layer], target_features[layer])
                loss += weight * layer_loss
        
        return loss
    
    def normalize_input(self, x):
        """å°†è¾“å…¥æ ‡å‡†åŒ–åˆ°[0,1]èŒƒå›´"""
        x_min, x_max = x.min(), x.max()
        if x_min >= -1.01 and x_max <= 1.01 and (x_min < -0.01 or x_max > 1.01):
            # å¯èƒ½æ˜¯tanhè¾“å‡ºï¼Œæ˜ å°„[-1,1] -> [0,1]
            return (x + 1.0) * 0.5
        else:
            # å·²ç»åœ¨[0,1]èŒƒå›´æˆ–éœ€è¦clamp
            return torch.clamp(x, 0.0, 1.0)
    
    def to_3channel(self, x):
        """å°†è¾“å…¥è½¬æ¢ä¸º3é€šé“"""
        if x.shape[1] == 1:
            # ç°åº¦è½¬RGBï¼šé‡å¤3æ¬¡
            return x.repeat(1, 3, 1, 1)
        elif x.shape[1] == 3:
            return x
        else:
            # å¤šé€šé“ï¼šå–å‰3ä¸ªé€šé“
            return x[:, :3, :, :]
    
    def extract_features(self, x):
        """æå–VGGç‰¹å¾"""
        features = {}
        temp_x = x
        
        for name, module in self.vgg._modules.items():
            temp_x = module(temp_x)
            for layer_name, layer_idx in self.layer_name_mapping.items():
                if name == layer_idx:
                    features[layer_name] = temp_x
        
        return features
