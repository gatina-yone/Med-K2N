#!/usr/bin/env python3
"""
General k->n inference script for MedicalMRI with SAM2 Distillation model.

Supports tasks like:
  - A2B, AB2CD, ABC2D, A2BCD, ABCD2ABCD, etc.

Inputs are discovered by MedicalMRI dataset via ABCD letter mapping to folders.
Loads a model-o                    if vis_mode == 'minmax':
                        # per-image min-max stretch like train.py normalize_img logic
                        pmin = float(pred_cpu.min())
                        pmax = float(pred_cpu.max())
                        if pmax > pmin:
                            pred_normalized = (pred_cpu - pmin) / (pmax - pmin)
                        else:
                            pred_normalized = pred_cpu - pmin
                        pred_normalized = torch.clamp(pred_normalized, 0.0, 1.0)
                    elif vis_mode == 'tanh-to-01':
                        # assume output in [-1,1], map to [0,1]  
                        pred_normalized = (pred_cpu + 1.0) * 0.5
                        pred_normalized = torch.clamp(pred_normalized, 0.0, 1.0)
                    else:
                        pred_normalized = torch.clamp(pred_cpu, 0.0, 1.0)saved by train.py (key: 'model_state_dict').

Outputs:
  Saves predicted images per target modality to --out-dir with filenames:
    {sample_id}_{slice}_{target_letter}.png
"""
import sys
import os
import argparse
import yaml
from pathlib import Path
from typing import List, Tuple, Optional

# Ensure project root is on sys.path so top-level package `semseg` is importable
sys.path.insert(0, '/data1/tempf/workplace/testmemsam/testSAM/semseg/models/sam2')

import torch
from torch.utils.data import DataLoader
from torch.cuda.amp.autocast_mode import autocast
import torchvision.utils as vutils
import numpy as np
from PIL import Image

from semseg.augmentations_mm import get_val_augmentation
from semseg.datasets.medicalmri import MedicalMRI
from semseg.models.sam2.sam2.build_sam import build_sam2
from semseg.models.sam2.sam2.sam_lora_image_encoder_seg import (
    SAM2LoRAConfig,
    create_sam2_lora_model,
)
from semseg.losses.accurate_loss import AccurateMetrics


def parse_task(task: Optional[str]) -> Tuple[Optional[List[str]], Optional[List[str]]]:
    """Parse spec like 'A2B', 'AB2CD', 'ABC2D', 'A2BCD'.

    Returns (input_modals, target_modals_list)
    """
    if not task:
        return None, None
    if '2' not in task:
        raise ValueError(f"Invalid task spec: {task}")
    left, right = task.split('2', 1)
    if not left or not right:
        raise ValueError(f"Invalid task spec: {task}")
    valid = set("ABCDEFGHIJKLMNOPQRSTUVWXYZ")
    inputs = [c for c in left if c in valid]
    targets = [c for c in right if c in valid]
    if len(inputs) == 0 or len(targets) == 0:
        raise ValueError(f"Invalid task spec (no inputs or targets): {task}")
    return inputs, targets


def to_uint8_image(t: torch.Tensor) -> torch.Tensor:
    """Clamp [0,1] and convert BCHW (or CHW) to uint8 for saving."""
    t = torch.nan_to_num(t, nan=0.0, posinf=1.0, neginf=0.0)
    t = t.clamp(0.0, 1.0)
    return (t * 255.0).to(torch.uint8)


def print_metrics_summary(total_metrics, valid_comparisons, sample_metrics, out_dir):
    """Print comprehensive metrics summary"""
    print("\n" + "="*60)
    print("ğŸ“Š INFERENCE METRICS SUMMARY")
    print("="*60)
    
    if valid_comparisons > 0:
        avg_psnr = total_metrics['psnr'] / valid_comparisons
        avg_ssim = total_metrics['ssim'] / valid_comparisons
        
        print(f"ğŸ¯ Overall Performance:")
        print(f"   â€¢ Samples processed: {valid_comparisons}")
        print(f"   â€¢ Average PSNR: {avg_psnr:.4f} dB")
        print(f"   â€¢ Average SSIM: {avg_ssim:.4f}")
        
        # Find best and worst samples
        if sample_metrics:
            psnr_values = [s['psnr'] for s in sample_metrics]
            ssim_values = [s['ssim'] for s in sample_metrics]
            
            best_psnr_idx = max(range(len(psnr_values)), key=lambda i: psnr_values[i])
            worst_psnr_idx = min(range(len(psnr_values)), key=lambda i: psnr_values[i])
            best_ssim_idx = max(range(len(ssim_values)), key=lambda i: ssim_values[i])
            worst_ssim_idx = min(range(len(ssim_values)), key=lambda i: ssim_values[i])
            
            print(f"\nğŸ“ˆ Performance Range:")
            print(f"   â€¢ PSNR: {min(psnr_values):.4f} - {max(psnr_values):.4f} dB")
            print(f"   â€¢ SSIM: {min(ssim_values):.4f} - {max(ssim_values):.4f}")
            
            print(f"\nğŸ† Best Performance:")
            print(f"   â€¢ Best PSNR: {sample_metrics[best_psnr_idx]['sample_id']} ({psnr_values[best_psnr_idx]:.4f} dB)")
            print(f"   â€¢ Best SSIM: {sample_metrics[best_ssim_idx]['sample_id']} ({ssim_values[best_ssim_idx]:.4f})")
            
            print(f"\nğŸ” Needs Review:")
            print(f"   â€¢ Lowest PSNR: {sample_metrics[worst_psnr_idx]['sample_id']} ({psnr_values[worst_psnr_idx]:.4f} dB)")
            print(f"   â€¢ Lowest SSIM: {sample_metrics[worst_ssim_idx]['sample_id']} ({ssim_values[worst_ssim_idx]:.4f})")
        
        # Performance evaluation
        print(f"\nâœ¨ Quality Assessment:")
        if avg_psnr >= 25:
            print(f"   â€¢ PSNR: Excellent ({avg_psnr:.1f} dB â‰¥ 25 dB)")
        elif avg_psnr >= 20:
            print(f"   â€¢ PSNR: Good ({avg_psnr:.1f} dB â‰¥ 20 dB)")
        else:
            print(f"   â€¢ PSNR: Needs improvement ({avg_psnr:.1f} dB < 20 dB)")
            
        if avg_ssim >= 0.9:
            print(f"   â€¢ SSIM: Excellent ({avg_ssim:.3f} â‰¥ 0.900)")
        elif avg_ssim >= 0.8:
            print(f"   â€¢ SSIM: Good ({avg_ssim:.3f} â‰¥ 0.800)")
        else:
            print(f"   â€¢ SSIM: Needs improvement ({avg_ssim:.3f} < 0.800)")
    else:
        print("âŒ No valid metric comparisons available")
    
    print(f"\nğŸ“ Results saved to: {out_dir}")
    print("="*60)


def main():
    parser = argparse.ArgumentParser(description="k->n Inference for MedicalMRI using SAM2 Distillation")
    parser.add_argument('--cfg', type=str, default='configs/medicalmri.yaml', help='YAML config path')
    parser.add_argument('--model-path', type=str, default=None, help='Path to model-only checkpoint (*.pth); if omitted, use cfg.TEST.MODEL_PATH')
    parser.add_argument('--task', type=str, default=None, help='Task spec, e.g., A2B, AB2CD, ABC2D, A2BCD')
    parser.add_argument('--data', type=str, default=None, help='Direct path to data directory containing modality folders (t1c, t1n, t2f, t2w)')
    parser.add_argument('--split', type=str, default='val', choices=['train','val'], help='Dataset split to run (ignored if --data is provided)')
    parser.add_argument('--batch-size', type=int, default=8, help='Batch size for inference')
    parser.add_argument('--device', type=str, default='cuda', help='Device, e.g., cuda or cuda:0 or cpu')
    parser.add_argument('--out-dir', type=str, default='outputs/inference', help='Base directory to save predictions')
    parser.add_argument('--custom-output', type=str, default=None, help='Custom output directory (overrides --out-dir and auto-generated subdirs)')
    parser.add_argument('--no-task-subdir', action='store_true', help='Do not create task-specific subdirectory (saves directly to --out-dir)')
    parser.add_argument('--output-prefix', type=str, default='', help='Prefix for output filenames')
    parser.add_argument('--amp', action='store_true', default=True, help='Enable autocast for inference')
    parser.add_argument('--num-workers', type=int, default=2)
    parser.add_argument('--max-samples', type=int, default=None, help='Limit number of samples to process (for quick demo)')
    parser.add_argument('--debug', action='store_true', help='Enable debug logging and save raw tensors')
    parser.add_argument('--visualize-mode', type=str, default='minmax', choices=['none','minmax','tanh-to-01','smooth'], help='How to map prediction tensor to [0,1] for saving images (default: minmax matches training visualization, smooth: apply anti-aliasing)')
    parser.add_argument('--anti-aliasing', action='store_true', help='Apply anti-aliasing filter to reduce grid artifacts')
    parser.add_argument('--save-raw', action='store_true', help='Also save raw tensor values without normalization')
    parser.add_argument('--output-size', type=int, nargs=2, default=None, help='Force output image size (height width), e.g., --output-size 256 256')
    args = parser.parse_args()

    # Load config
    with open(args.cfg, 'r') as f:
        cfg = yaml.safe_load(f)

    # Override dataset task if provided
    # Priority: --task argument > TEST.TASK > DATASET.TASK
    task_spec = args.task or cfg.get('TEST', {}).get('TASK') or cfg.get('DATASET', {}).get('TASK')
    inputs, targets = parse_task(task_spec) if task_spec else (None, None)
    
    # Use TEST configuration if available, fallback to DATASET
    test_cfg = cfg.get('TEST', {})
    ds_cfg = cfg.get('DATASET', {})
    
    # Override with direct data path if provided
    if args.data:
        print(f"ğŸ”§ ä½¿ç”¨ç›´æ¥æ•°æ®è·¯å¾„: {args.data}")
        # For direct data path, we assume the structure is:
        # /path/to/data/
        # â”œâ”€â”€ t1c/
        # â”œâ”€â”€ t1n/  
        # â”œâ”€â”€ t2f/
        # â””â”€â”€ t2w/
        ds_cfg['ROOT'] = args.data
        # Use a dummy split since we're pointing directly to modality folders
        args.split = ''  # Empty split for direct path
        print(f"   æ¨¡æ€æ–‡ä»¶å¤¹: {args.data}/*")
    elif test_cfg:
        print("ğŸ”§ ä½¿ç”¨TESTé…ç½®è¿›è¡Œæ¨ç†...")
        # Override dataset root if specified in TEST
        if 'ROOT' in test_cfg:
            ds_cfg['ROOT'] = test_cfg['ROOT']
            print(f"   æ•°æ®è·¯å¾„: {test_cfg['ROOT']}")
        
        # Override split if specified in TEST
        if 'SPLIT' in test_cfg:
            args.split = test_cfg['SPLIT']
            
        # Override batch size if specified in TEST  
        if 'BATCH_SIZE' in test_cfg:
            args.batch_size = test_cfg['BATCH_SIZE']
    
    # Override modals and targets from TEST or task specification
    if test_cfg and not args.data:  # Only use TEST modals if not using direct data path
        # Override modals if specified in TEST
        if 'MODALS' in test_cfg:
            ds_cfg['MODALS'] = test_cfg['MODALS']
        elif inputs is not None:
            ds_cfg['MODALS'] = inputs
            
        # Override target modal if specified in TEST
        if 'TARGET_MODAL' in test_cfg:
            ds_cfg['TARGET_MODAL'] = test_cfg['TARGET_MODAL']
        elif targets is not None:
            if len(targets) > 1:
                ds_cfg['TARGET_MODAL'] = targets
            else:
                ds_cfg['TARGET_MODAL'] = targets[0]
    else:
        # Use task specification or fallback logic
        if inputs is not None:
            ds_cfg['MODALS'] = inputs
        if targets is not None:
            if len(targets) > 1:
                ds_cfg['TARGET_MODAL'] = targets
            else:
                ds_cfg['TARGET_MODAL'] = targets[0]
    
    print(f"   ä»»åŠ¡: {task_spec or 'ABCD2ABCD'}")
    print(f"   è¾“å…¥æ¨¡æ€: {ds_cfg.get('MODALS', ['A', 'B', 'C', 'D'])}")
    print(f"   ç›®æ ‡æ¨¡æ€: {ds_cfg.get('TARGET_MODAL', ['A', 'B', 'C', 'D'])}")
    print(f"   æ•°æ®åˆ’åˆ†: {args.split if args.split else 'ç›´æ¥è·¯å¾„æ¨¡å¼'}")

    device = torch.device(args.device)

    # Dataset & loader
    ds_cfg = cfg['DATASET']
    
    # ğŸ”§ å¯ä»¥åœ¨è¿™é‡Œä¸´æ—¶ä¿®æ”¹æ•°æ®è·¯å¾„
    # ds_cfg['ROOT'] = '/path/to/your/test/data'  # å–æ¶ˆæ³¨é‡Šå¹¶ä¿®æ”¹è·¯å¾„
    
    transform = get_val_augmentation(cfg.get('EVAL', {}).get('IMAGE_SIZE', [256, 256]))
    dataset = MedicalMRI(
        root=ds_cfg.get('ROOT'),
        split=args.split,
        transform=transform,
        modals=ds_cfg.get('MODALS'),
        target_modal=ds_cfg.get('TARGET_MODAL'),
    )
    loader = DataLoader(
        dataset,
        batch_size=max(1, int(args.batch_size)),
        shuffle=False,
        num_workers=int(args.num_workers),
        pin_memory=False,
        persistent_workers=False,
    )

    # Build teacher/student SAM2 backbones (use same defaults as train.py)
    model_cfg_file = "sam2_hiera_b+_256.yaml"
    backbone_checkpoint = cfg.get('MODEL', {}).get('PRETRAINED', "/data1/tempf/sam2.1_hiera_base_plus.pt")
    teacher = build_sam2(model_cfg_file, backbone_checkpoint, device=str(device))
    student = build_sam2(model_cfg_file, backbone_checkpoint, device=str(device))

    # Prepare LoRA/distill config and try to align it with the checkpoint structure.
    lora_cfg = SAM2LoRAConfig()
    
    # ğŸ”§ Apply YAML configuration updates (feat_dim=256 optimization)
    try:
        lora_cfg.update_from_yaml_config(cfg)
        print(f"âœ… å·²ä»YAMLé…ç½®æ›´æ–°å‚æ•°: feat_dim={lora_cfg.feat_dim}, embed_dim={lora_cfg.embed_dim}")
    except Exception as e:
        print(f"âš ï¸  YAMLé…ç½®æ›´æ–°å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
        # æ‰‹åŠ¨è®¾ç½®feat_dim=256ä¼˜åŒ–ç‰ˆæœ¬
        lora_cfg.feat_dim = 256
        lora_cfg.embed_dim = 128
        print(f"ğŸ”§ æ‰‹åŠ¨è®¾ç½®ä¼˜åŒ–å‚æ•°: feat_dim={lora_cfg.feat_dim}, embed_dim={lora_cfg.embed_dim}")
    
    # Match number of output frames to number of targets in k->n (initial guess)
    n_targets = 1  # é»˜è®¤å€¼
    try:
        tgt_cfg = ds_cfg.get('TARGET_MODAL', None)
        n_targets = len(tgt_cfg) if isinstance(tgt_cfg, list) else 1
        if n_targets > 0:
            lora_cfg.num_output_frames = n_targets
            lora_cfg.num_generator_heads = n_targets
        print(f"ğŸ“Š åˆå§‹é…ç½®: num_output_frames={lora_cfg.num_output_frames}, num_generator_heads={lora_cfg.num_generator_heads}")
    except Exception:
        pass

    # Resolve model path early so we can inspect checkpoint and detect generator head count
    model_path = args.model_path or cfg.get('TEST', {}).get('MODEL_PATH')
    if not model_path:
        raise ValueError("Model path not provided and cfg.TEST.MODEL_PATH missing.")
    ckpt_path = Path(model_path)
    if not ckpt_path.exists():
        raise FileNotFoundError(f"Checkpoint not found: {ckpt_path}")

    # Load checkpoint (model-only or full) to inspect keys before constructing model
    ckpt = torch.load(str(ckpt_path), map_location=device)
    state = ckpt.get('model_state_dict', ckpt)

    # Detect number of generation heads from state keys (if any)
    def _detect_num_generator_heads(state_dict):
        head_indices = set()
        for k in state_dict.keys():
            if 'generation_heads.' in k:
                try:
                    suffix = k.split('generation_heads.')[1]
                    idx = suffix.split('.')[0]
                    if idx.isdigit():
                        head_indices.add(int(idx))
                except Exception:
                    continue
        if len(head_indices) > 0:
            return max(head_indices) + 1
        return None
    
    def _detect_output_channels(state_dict):
        """æ£€æµ‹temporal_refinerçš„è¾“å‡ºé€šé“æ•°"""
        for k, v in state_dict.items():
            if 'temporal_refiner' in k and '.6.weight' in k and len(v.shape) == 4:
                return v.shape[0]  # è¾“å‡ºé€šé“æ•°
        return None

    detected_heads = _detect_num_generator_heads(state)
    detected_channels = _detect_output_channels(state)
    
    print(f"ğŸ” æ£€æŸ¥ç‚¹åˆ†æ: æ£€æµ‹åˆ° {detected_heads} ä¸ªç”Ÿæˆå¤´, {detected_channels} ä¸ªè¾“å‡ºé€šé“")
    
    if detected_heads is not None:
        # Align generator head count and output frames with checkpoint
        print(f"Detected {detected_heads} generator heads in checkpoint; aligning model config.")
        lora_cfg.num_generator_heads = detected_heads
        # Ensure num_output_frames is at least as many as generator heads to avoid shape mismatch
        if lora_cfg.num_output_frames < detected_heads:
            lora_cfg.num_output_frames = detected_heads
    else:
        print("No explicit generator head indices detected in checkpoint; using config/defaults.")
        
    # è°ƒæ•´è¾“å‡ºé€šé“æ•°ä»¥åŒ¹é…å½“å‰ä»»åŠ¡
    original_n_targets = n_targets
    if detected_channels is not None and detected_channels != n_targets:
        print(f"âš ï¸  æ£€æŸ¥ç‚¹è¾“å‡ºé€šé“({detected_channels}) != å½“å‰ä»»åŠ¡é€šé“({n_targets}), å°†è¿›è¡Œé€‚é…")
        # é‡è¦ï¼šè°ƒæ•´æ¨¡å‹é…ç½®ä»¥åŒ¹é…æ£€æŸ¥ç‚¹ï¼Œè€Œä¸æ˜¯ç›¸å
        print(f"ğŸ”§ è°ƒæ•´æ¨¡å‹é…ç½®: ä½¿ç”¨æ£€æŸ¥ç‚¹çš„ {detected_channels} é€šé“é…ç½®")
        
        # æ ¹æ®temporal_refinerçš„å½¢çŠ¶åˆ†æï¼š
        # - temporal_refiner.0.weight: [64, 12, 3, 3] -> æœŸæœ›è¾“å…¥12é€šé“
        # - temporal_refiner.6.weight: [12, 32, 3, 3] -> è¾“å‡º12é€šé“
        # temporal_refinerè¾“å…¥é€šé“ = 3 * output_framesï¼Œæ‰€ä»¥ï¼š
        # 12 = 3 * output_frames => output_frames = 4
        
        # è®¾ç½®æ­£ç¡®çš„è¾“å‡ºå¸§æ•°å’Œç”Ÿæˆå™¨å¤´æ•°
        lora_cfg.num_output_frames = 4  # ä½¿å¾—3*4=12åŒ¹é…æ£€æŸ¥ç‚¹è¾“å…¥é€šé“
        lora_cfg.num_generator_heads = 4  # 4ä¸ªç”Ÿæˆå¤´
        
        print(f"ğŸ“Š æ ¹æ®temporal_refinerç»“æ„è°ƒæ•´: output_frames={lora_cfg.num_output_frames} (3Ã—4=12è¾“å…¥é€šé“)")
        print(f"ğŸ“Š æ›´æ–°é…ç½®: num_output_frames={lora_cfg.num_output_frames}, num_generator_heads={lora_cfg.num_generator_heads}")

    # Now create model with aligned config
    print(f"ğŸš€ åˆ›å»ºä¼˜åŒ–ç‰ˆMedK2Næ¨¡å‹ (feat_dim={lora_cfg.feat_dim})")
    model = create_sam2_lora_model(teacher_model=teacher, student_model=student, config=lora_cfg)
    model = model.to(device)
    model.eval()
    
    # ğŸ“Š ç»Ÿè®¡æ¨¡å‹å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    sam2_params = sum(p.numel() for n, p in model.named_parameters() if p.requires_grad and not any(x in n for x in ['medk2n', 'generation_head']))
    medk2n_params = total_params - sam2_params
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
    print(f"   SAM2å‚æ•°: {sam2_params:,}")
    print(f"   MedK2Nå‚æ•°: {medk2n_params:,}")  
    print(f"   æ€»å‚æ•°: {total_params:,}")
    print(f"   é¢„æœŸå‡å°‘: ~{(111_000_000 - total_params) / 111_000_000 * 100:.1f}% (ç›¸æ¯”åŸç‰ˆ111M)")

    # Load weights without channel adaptation since model now matches checkpoint
    missing, unexpected = model.load_state_dict(state, strict=False)
    print(f"ğŸ“¦ åŠ è½½æ£€æŸ¥ç‚¹: {ckpt_path}")
    if missing:
        print(f"  âš ï¸  ç¼ºå¤±é”®: {len(missing)} (æ˜¾ç¤ºå‰10ä¸ª) -> {missing[:10]}")
    if unexpected:
        print(f"  âš ï¸  æ„å¤–é”®: {len(unexpected)} (æ˜¾ç¤ºå‰10ä¸ª) -> {unexpected[:10]}")
    
    # éªŒè¯å…³é”®MedK2Nç»„ä»¶æ˜¯å¦æ­£ç¡®åŠ è½½
    medk2n_loaded = sum(1 for k in state.keys() if any(x in k.lower() for x in ['preweight', 'threshold', 'resfusion', 'taskhead']))
    print(f"  âœ… MedK2Nç»„ä»¶åŠ è½½: {medk2n_loaded}ä¸ªæ ¸å¿ƒç»„ä»¶")

    # Prepare output directory
    # Compose a run name using inputs/targets like in training
    in_letters = ds_cfg.get('MODALS') or []
    tgt_letters = ds_cfg.get('TARGET_MODAL')
    if isinstance(tgt_letters, list):
        tgt_str = ''.join(tgt_letters)
    else:
        tgt_str = tgt_letters or ''
    run_name = (''.join(in_letters) or 'A') + '2' + (tgt_str or (in_letters[-1] if in_letters else 'B'))
    
    # æ™ºèƒ½è¾“å‡ºè·¯å¾„é€‰æ‹©
    if args.custom_output:
        # ç”¨æˆ·æŒ‡å®šäº†å®Œå…¨è‡ªå®šä¹‰çš„è¾“å‡ºè·¯å¾„
        out_dir = Path(args.custom_output)
        print(f"ä½¿ç”¨è‡ªå®šä¹‰è¾“å‡ºè·¯å¾„: {out_dir}")
    elif args.no_task_subdir:
        # ç›´æ¥ä½¿ç”¨åŸºç¡€è¾“å‡ºç›®å½•ï¼Œä¸åˆ›å»ºä»»åŠ¡å­ç›®å½•
        out_dir = Path(args.out_dir)
        print(f"ä½¿ç”¨åŸºç¡€è¾“å‡ºè·¯å¾„: {out_dir}")
    else:
        # é»˜è®¤è¡Œä¸ºï¼šåŸºç¡€ç›®å½• + ä»»åŠ¡å­ç›®å½•
        out_dir = Path(args.out_dir) / run_name
        print(f"ä½¿ç”¨ä»»åŠ¡ç‰¹å®šè¾“å‡ºè·¯å¾„: {out_dir}")
    
    out_dir.mkdir(parents=True, exist_ok=True)
    debug_dir = out_dir / 'debug'
    if args.debug:
        debug_dir.mkdir(parents=True, exist_ok=True)

    # For naming, we need to know sample ids and slice numbers
    samples = list(dataset.samples)
    channels_per_img = 3

    print(f"ğŸš€ è¿è¡Œfeat_dim={lora_cfg.feat_dim}ä¼˜åŒ–ç‰ˆæ¨ç†: {len(dataset)} æ ·æœ¬ -> {out_dir}")
    
    # æ€§èƒ½ç›‘æ§
    import time
    start_time = time.time()
    batch_times = []
    
    global_index = 0
    processed = 0
    
    # Initialize metrics tracking
    total_metrics = {'psnr': 0.0, 'ssim': 0.0}
    valid_comparisons = 0
    sample_metrics = []  # Store per-sample metrics
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(loader):
            batch_start = time.time()
            
            inputs_batch, targets_batch = batch  # targets are needed for metrics calculation
            # Move inputs to device
            inputs_batch = [x.to(device, non_blocking=True) for x in inputs_batch]
            # Move targets to device for metrics
            if isinstance(targets_batch, list):
                targets_batch = [t.to(device, non_blocking=True) for t in targets_batch]
            else:
                targets_batch = targets_batch.to(device, non_blocking=True)

            # ğŸ”¥ MedK2Næ¨ç† (feat_dim=256ä¼˜åŒ–ç‰ˆæœ¬)
            with autocast(enabled=bool(args.amp)):
                out_gen, _out_feat, out_dict = model(inputs_batch, multimask_output=True)
            
            # ğŸ”§ è°ƒæ•´è¾“å‡ºå°ºå¯¸åˆ°æœŸæœ›çš„256x256
            if args.output_size:
                target_size = args.output_size
            else:
                target_size = cfg.get('EVAL', {}).get('IMAGE_SIZE', [256, 256])
            
            if isinstance(out_gen, list):
                resized_out_gen = []
                for pred in out_gen:
                    if pred is not None and pred.shape[-2:] != tuple(target_size):
                        import torch.nn.functional as F
                        pred_resized = F.interpolate(
                            pred, 
                            size=target_size, 
                            mode='bilinear', 
                            align_corners=False
                        )
                        resized_out_gen.append(pred_resized)
                        if args.debug and batch_idx == 0:
                            print(f"ğŸ”§ è°ƒæ•´è¾“å‡ºå°ºå¯¸: {pred.shape[-2:]} -> {target_size}")
                    else:
                        resized_out_gen.append(pred)
                out_gen = resized_out_gen
            elif out_gen is not None and out_gen.shape[-2:] != tuple(target_size):
                import torch.nn.functional as F
                out_gen = F.interpolate(
                    out_gen, 
                    size=target_size, 
                    mode='bilinear', 
                    align_corners=False
                )
                if args.debug and batch_idx == 0:
                    print(f"ğŸ”§ è°ƒæ•´è¾“å‡ºå°ºå¯¸: {out_gen.shape[-2:]} -> {target_size}")
            
            batch_time = time.time() - batch_start
            batch_times.append(batch_time)
            
            if batch_idx % 10 == 0:
                avg_batch_time = sum(batch_times) / len(batch_times)
                print(f"  æ‰¹æ¬¡ {batch_idx}: {batch_time:.2f}s (å¹³å‡: {avg_batch_time:.2f}s/batch)")

            # Normalize output list
            preds: List[Optional[torch.Tensor]]
            preds = out_gen if isinstance(out_gen, list) else [out_gen]
            # Fallback to outputs in dict if needed
            if all(p is None for p in preds):
                try:
                    gen_from_dict = None
                    if isinstance(out_dict, dict):
                        gen_from_dict = out_dict.get('generated_rgb_frames', None)
                    if gen_from_dict is not None:
                        preds = gen_from_dict if isinstance(gen_from_dict, list) else [gen_from_dict]
                except Exception:
                    pass

            # Ensure we have at least one tensor
            if len(preds) == 0 or all(p is None for p in preds):
                # Skip saving if nothing produced
                bsz = inputs_batch[0].shape[0]
                global_index += bsz
                continue

            # Determine per-batch size
            bsz = None
            for p in preds:
                if isinstance(p, torch.Tensor):
                    bsz = p.shape[0]
                    break
            if bsz is None:
                bsz = inputs_batch[0].shape[0]

            # Build target letters list for naming and determine which heads to use
            if isinstance(ds_cfg.get('TARGET_MODAL'), list):
                tgt_letters_list = list(ds_cfg['TARGET_MODAL'])
            elif ds_cfg.get('TARGET_MODAL') is None:
                # If unspecified (e.g., ABCD2ABCD), use all used modals
                tgt_letters_list = list(dataset.used_modals)
            else:
                tgt_letters_list = [str(ds_cfg['TARGET_MODAL'])]

            # Map target letters to head indices based on ABCD mapping
            # Training used ABCD order, so A=head0, B=head1, C=head2, D=head3
            modal_to_head = {'A': 0, 'B': 1, 'C': 2, 'D': 3}
            target_head_indices = []
            for tgt_letter in tgt_letters_list:
                if tgt_letter in modal_to_head:
                    target_head_indices.append(modal_to_head[tgt_letter])
                else:
                    # Fallback for unknown modals
                    target_head_indices.append(len(target_head_indices))
            
            # å¦‚æœæ£€æŸ¥ç‚¹é€šé“æ•°è¶…è¿‡å®é™…éœ€è¦çš„ç›®æ ‡æ•°é‡ï¼Œåªä½¿ç”¨éœ€è¦çš„å¤´
            print(f"ğŸ“Š æ¨ç†é…ç½®: æ¨¡å‹æœ‰{len(preds)}ä¸ªå¤´ï¼Œéœ€è¦è¾“å‡º{len(tgt_letters_list)}ä¸ªç›®æ ‡({tgt_letters_list})")

            # Save each sample in batch
            for bi in range(bsz):
                if global_index + bi >= len(samples):
                    continue
                sample_id, slice_num = samples[global_index + bi]
                
                # Calculate metrics for this sample
                sample_psnr_sum = 0.0
                sample_ssim_sum = 0.0
                sample_valid_targets = 0

                # Only process the heads corresponding to target modalities
                for idx, tgt_letter in enumerate(tgt_letters_list):
                    head_idx = target_head_indices[idx] if idx < len(target_head_indices) else idx
                    
                    # Skip if this head doesn't exist in predictions
                    if head_idx >= len(preds) or preds[head_idx] is None:
                        continue
                    
                    pred = preds[head_idx]

                    # Get corresponding target for metrics calculation
                    target_tensor = None
                    if isinstance(targets_batch, list):
                        if idx < len(targets_batch):
                            target_tensor = targets_batch[idx][bi:bi+1]  # Keep batch dimension
                    else:
                        target_tensor = targets_batch[bi:bi+1]  # Keep batch dimension

                    # Calculate PSNR and SSIM if target is available
                    if target_tensor is not None and pred is not None:
                        pred_for_metrics = pred[bi:bi+1]  # Keep batch dimension
                        try:
                            # Handle size mismatch by resizing target to match prediction
                            if pred_for_metrics.shape != target_tensor.shape:
                                import torch.nn.functional as F
                                if pred_for_metrics.shape[-2:] != target_tensor.shape[-2:]:  # Different spatial dimensions
                                    # Resize target to match prediction size
                                    target_tensor = F.interpolate(
                                        target_tensor, 
                                        size=pred_for_metrics.shape[-2:], 
                                        mode='bilinear', 
                                        align_corners=False
                                    )
                                    if args.debug:
                                        print(f"Resized target from {target_tensor.shape} to {pred_for_metrics.shape} for metrics")
                            
                            # Ensure both tensors are in [0,1] range
                            pred_clamped = torch.clamp(pred_for_metrics, 0, 1)
                            target_clamped = torch.clamp(target_tensor, 0, 1)
                            
                            psnr_val = AccurateMetrics.calculate_psnr(pred_clamped, target_clamped, data_range=1.0)
                            ssim_val = AccurateMetrics.calculate_ssim(pred_clamped, target_clamped, data_range=1.0)
                            
                            sample_psnr_sum += psnr_val
                            sample_ssim_sum += ssim_val
                            sample_valid_targets += 1
                            
                            if args.debug:
                                print(f"Sample {sample_id}_{slice_num}_{tgt_letter}: PSNR={psnr_val:.4f}, SSIM={ssim_val:.4f}")
                        except Exception as e:
                            if args.debug:
                                print(f"Failed to calculate metrics for {sample_id}_{slice_num}_{tgt_letter}: {e}")

                    if pred is not None:
                        # ä¿å­˜é¢„æµ‹å›¾åƒ
                        img = pred[bi]  # CHW
                        # Convert to uint8 image grid (single image)
                        # If channels != 3, try to map or replicate
                        if img.dim() != 3:
                            continue
                        if img.shape[0] != channels_per_img:
                            # replicate first channel to 3
                            if img.shape[0] == 1:
                                img = img.repeat(3, 1, 1)
                            else:
                                img = img[:3, ...]
                        # Apply visualization mapping based on train.py normalize_img logic
                        vis_mode = (args.visualize_mode or 'none')
                        pred_cpu = pred[bi].detach().cpu().float()
                        
                        # ğŸ”§ æ·»åŠ æŠ—é”¯é½¿/å¹³æ»‘å¤„ç†é€‰é¡¹
                        if args.anti_aliasing or vis_mode == 'smooth':
                            # åº”ç”¨é«˜æ–¯æ¨¡ç³Šå‡å°‘æ ¼çº¹ä¼ªå½±
                            import torch.nn.functional as F
                            # åˆ›å»ºé«˜æ–¯æ ¸
                            kernel_size = 3
                            sigma = 0.5
                            channels = pred_cpu.shape[0]
                            
                            # åˆ›å»º1Dé«˜æ–¯æ ¸
                            x = torch.arange(kernel_size).float() - kernel_size // 2
                            gauss = torch.exp(-0.5 * (x / sigma).pow(2))
                            gauss = gauss / gauss.sum()
                            
                            # åˆ›å»º2Dåˆ†ç¦»å¼é«˜æ–¯æ ¸
                            gauss_kernel = gauss.view(1, 1, -1) * gauss.view(1, -1, 1)
                            gauss_kernel = gauss_kernel.expand(channels, 1, kernel_size, kernel_size)
                            
                            # åº”ç”¨é«˜æ–¯æ¨¡ç³Š
                            pred_cpu_padded = F.pad(pred_cpu.unsqueeze(0), (kernel_size//2, kernel_size//2, kernel_size//2, kernel_size//2), mode='reflect')
                            pred_cpu = F.conv2d(pred_cpu_padded, gauss_kernel, groups=channels, padding=0).squeeze(0)
                            
                            if args.debug:
                                print(f"Applied anti-aliasing filter to {sample_id}_{slice_num}_{tgt_letter}")
                        
                        if vis_mode == 'minmax' or vis_mode == 'smooth':
                            # per-image min-max stretch like train.py normalize_img
                            pmin = float(pred_cpu.min())
                            pmax = float(pred_cpu.max())
                            if pmax > pmin:
                                pred_normalized = (pred_cpu - pmin) / (pmax - pmin)
                            else:
                                pred_normalized = pred_cpu - pmin
                            pred_normalized = torch.clamp(pred_normalized, 0.0, 1.0)
                        elif vis_mode == 'tanh-to-01':
                            # assume output in [-1,1], map to [0,1]
                            pred_normalized = (pred_cpu + 1.0) * 0.5
                            pred_normalized = torch.clamp(pred_normalized, 0.0, 1.0)
                        else:
                            pred_normalized = torch.clamp(pred_cpu, 0.0, 1.0)
                        
                        # Use matplotlib save like train.py instead of torchvision
                        # Convert to single channel (average across RGB) for grayscale display
                        if pred_normalized.shape[0] == 3:
                            pred_gray = pred_normalized.mean(dim=0)  # HW
                        else:
                            pred_gray = pred_normalized[0]  # HW
                        
                        # Save pure 256x256 image without any titles or axes
                        # Convert to PIL Image and save directly
                        
                        # Ensure the image is exactly 256x256
                        pred_array = (pred_gray.numpy() * 255).astype(np.uint8)
                        if pred_array.shape != (256, 256):
                            pred_array = np.resize(pred_array, (256, 256))
                        
                        # æ„å»ºæ–‡ä»¶åï¼ˆæ”¯æŒå‰ç¼€ï¼‰
                        prefix = args.output_prefix + "_" if args.output_prefix else ""
                        save_path = out_dir / f"{prefix}{sample_id}_{slice_num}_{tgt_letter}_pred.png"
                        
                        # Save as pure grayscale PNG
                        img = Image.fromarray(pred_array, mode='L')
                        img.save(str(save_path))
                        
                        # ğŸ†• ä¿å­˜å¯¹åº”çš„å‚è€ƒå›¾åƒï¼ˆGround Truthï¼‰
                        if target_tensor is not None:
                            try:
                                # å¤„ç†å‚è€ƒå›¾åƒï¼Œåº”ç”¨ç›¸åŒçš„å¯è§†åŒ–è®¾ç½®
                                target_cpu = target_tensor[0].detach().cpu().float()  # å»æ‰æ‰¹æ¬¡ç»´åº¦
                                
                                # å¦‚æœå°ºå¯¸ä¸åŒ¹é…ï¼Œè°ƒæ•´å‚è€ƒå›¾åƒå°ºå¯¸åˆ°é¢„æµ‹å›¾åƒå°ºå¯¸
                                if target_cpu.shape != pred_cpu.shape:
                                    import torch.nn.functional as F
                                    target_cpu = F.interpolate(
                                        target_cpu.unsqueeze(0), 
                                        size=pred_cpu.shape[-2:], 
                                        mode='bilinear', 
                                        align_corners=False
                                    ).squeeze(0)
                                
                                # åº”ç”¨ç›¸åŒçš„å¯è§†åŒ–æ˜ å°„
                                if vis_mode == 'minmax':
                                    tmin = float(target_cpu.min())
                                    tmax = float(target_cpu.max())
                                    if tmax > tmin:
                                        target_normalized = (target_cpu - tmin) / (tmax - tmin)
                                    else:
                                        target_normalized = target_cpu - tmin
                                    target_normalized = torch.clamp(target_normalized, 0.0, 1.0)
                                elif vis_mode == 'tanh-to-01':
                                    target_normalized = (target_cpu + 1.0) * 0.5
                                    target_normalized = torch.clamp(target_normalized, 0.0, 1.0)
                                else:
                                    target_normalized = torch.clamp(target_cpu, 0.0, 1.0)
                                
                                # è½¬æ¢ä¸ºç°åº¦å›¾
                                if target_normalized.shape[0] == 3:
                                    target_gray = target_normalized.mean(dim=0)  # HW
                                else:
                                    target_gray = target_normalized[0]  # HW
                                
                                # ä¿å­˜çº¯256x256å‚è€ƒå›¾åƒï¼Œæ— æ ‡é¢˜å’Œåæ ‡è½´
                                target_array = (target_gray.numpy() * 255).astype(np.uint8)
                                if target_array.shape != (256, 256):
                                    target_array = np.resize(target_array, (256, 256))
                                
                                gt_save_path = out_dir / f"{prefix}{sample_id}_{slice_num}_{tgt_letter}_gt.png"
                                gt_img = Image.fromarray(target_array, mode='L')
                                gt_img.save(str(gt_save_path))
                                
                                # ğŸ†• åˆ›å»º256x512å¯¹æ¯”å›¾åƒï¼ˆé¢„æµ‹ | å‚è€ƒï¼‰
                                comparison_array = np.zeros((256, 512), dtype=np.uint8)
                                comparison_array[:, :256] = (pred_gray.numpy() * 255).astype(np.uint8)  # å·¦åŠè¾¹ï¼šé¢„æµ‹
                                comparison_array[:, 256:] = (target_gray.numpy() * 255).astype(np.uint8)  # å³åŠè¾¹ï¼šå‚è€ƒ
                                
                                # ä¿å­˜å¯¹æ¯”å›¾åƒ
                                comparison_save_path = out_dir / f"{prefix}{sample_id}_{slice_num}_{tgt_letter}_comparison.png"
                                comparison_img = Image.fromarray(comparison_array, mode='L')
                                comparison_img.save(str(comparison_save_path))
                                
                                if args.debug:
                                    print(f"âœ… å·²ä¿å­˜ {tgt_letter}: é¢„æµ‹å›¾åƒ, å‚è€ƒå›¾åƒ, å¯¹æ¯”å›¾åƒ")
                                    
                            except Exception as e:
                                if args.debug:
                                    print(f"âš ï¸  ä¿å­˜å‚è€ƒå›¾åƒå¤±è´¥ {sample_id}_{slice_num}_{tgt_letter}: {e}")
                        
                        # Debug: print and save stats/raw tensor
                        if args.debug:
                            try:
                                pred_tensor = pred[bi].detach().cpu()
                                mn = float(pred_tensor.min())
                                mx = float(pred_tensor.max())
                                mean = float(pred_tensor.mean())
                                with open(debug_dir / f"{sample_id}_{slice_num}_{tgt_letter}_stats.txt", 'w') as sf:
                                    sf.write(f"min={mn}\nmax={mx}\nmean={mean}\nshape={tuple(pred_tensor.shape)}\n")
                                # save raw tensor for inspection
                                torch.save(pred_tensor, debug_dir / f"{sample_id}_{slice_num}_{tgt_letter}.pt")
                            except Exception as _e:
                                print(f"[debug] failed to dump tensor: {_e}")
                        processed += 1
                        if args.max_samples is not None and processed >= int(args.max_samples):
                            print(f"Reached max-samples={args.max_samples}, stopping early.")
                            # Record final sample metrics before early exit
                            if sample_valid_targets > 0:
                                avg_sample_psnr = sample_psnr_sum / sample_valid_targets
                                avg_sample_ssim = sample_ssim_sum / sample_valid_targets
                                sample_metrics.append({
                                    'sample_id': f"{sample_id}_{slice_num}",
                                    'psnr': avg_sample_psnr,
                                    'ssim': avg_sample_ssim,
                                    'num_targets': sample_valid_targets
                                })
                                total_metrics['psnr'] += avg_sample_psnr
                                total_metrics['ssim'] += avg_sample_ssim
                                valid_comparisons += 1
                            # Print metrics summary before exit
                            print_metrics_summary(total_metrics, valid_comparisons, sample_metrics, out_dir)
                            return
                
                # Record sample-level metrics
                if sample_valid_targets > 0:
                    avg_sample_psnr = sample_psnr_sum / sample_valid_targets
                    avg_sample_ssim = sample_ssim_sum / sample_valid_targets
                    sample_metrics.append({
                        'sample_id': f"{sample_id}_{slice_num}",
                        'psnr': avg_sample_psnr,
                        'ssim': avg_sample_ssim,
                        'num_targets': sample_valid_targets
                    })
                    total_metrics['psnr'] += avg_sample_psnr
                    total_metrics['ssim'] += avg_sample_ssim
                    valid_comparisons += 1
                    
                    print(f"âœ“ {sample_id}_{slice_num}: PSNR={avg_sample_psnr:.4f}, SSIM={avg_sample_ssim:.4f} ({sample_valid_targets} targets)")

            global_index += bsz

    total_time = time.time() - start_time
    avg_time_per_sample = total_time / max(processed, 1)
    avg_batch_time = sum(batch_times) / max(len(batch_times), 1)
    
    print(f"ğŸ æ¨ç†å®Œæˆ!")
    print(f"ğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
    print(f"   æ€»è€—æ—¶: {total_time:.1f}s")
    print(f"   å¹³å‡æ¯æ ·æœ¬: {avg_time_per_sample:.3f}s")
    print(f"   å¹³å‡æ¯æ‰¹æ¬¡: {avg_batch_time:.2f}s")
    print(f"   å¤„ç†æ ·æœ¬æ•°: {processed}")
    print(f"âœ… é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³: {out_dir}")
    
    # Print final metrics summary
    print_metrics_summary(total_metrics, valid_comparisons, sample_metrics, out_dir)


if __name__ == '__main__':
    main()

# ============================================================================
# ğŸš€ feat_dim=256 ä¼˜åŒ–ç‰ˆæœ¬ä½¿ç”¨ç¤ºä¾‹
# ============================================================================

# 1ï¸âƒ£ åŸºç¡€æ¨ç† (ä½¿ç”¨ä¼˜åŒ–åçš„76Må‚æ•°æ¨¡å‹ï¼Œè¾“å‡º256x256)
# python infer_k2n.py --cfg configs/medicalmri.yaml --split val --device cuda:0 --visualize-mode minmax --output-size 256 256

# 2ï¸âƒ£ æ€§èƒ½å¯¹æ¯”æµ‹è¯• (é™åˆ¶æ ·æœ¬æ•°é‡å¿«é€ŸéªŒè¯ï¼Œå¼ºåˆ¶256x256è¾“å‡º)
# python infer_k2n.py --cfg configs/medicalmri.yaml --max-samples 50 --debug --visualize-mode minmax --output-size 256 256

# 3ï¸âƒ£ ç‰¹å®šä»»åŠ¡æ¨ç† (256x256è¾“å‡º)
# python infer_k2n.py --cfg configs/medicalmri.yaml --task A2B --visualize-mode minmax --output-size 256 256

# 4ï¸âƒ£ å…¨é‡éªŒè¯é›†è¯„ä¼° (å»ºè®®åœ¨feat_dim=256ä¼˜åŒ–åä½¿ç”¨ï¼Œ256x256è¾“å‡º)
# python infer_k2n.py --cfg configs/medicalmri.yaml --split val --device cuda:0 --visualize-mode minmax --output-size 256 256 --out-dir outputs/feat256_evaluation_256

# 5ï¸âƒ£ è°ƒè¯•æ¨¡å¼ (ä¿å­˜ä¸­é—´ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯ï¼Œ256x256è¾“å‡º)
# python infer_k2n.py --cfg configs/medicalmri.yaml --debug --max-samples 10 --output-size 256 256 --out-dir outputs/debug_feat256_256

# ğŸ”§ æ ¼çº¹é—®é¢˜è¯Šæ–­å’Œè§£å†³æ–¹æ¡ˆ (256x256è¾“å‡º + æŠ—é”¯é½¿)
# python infer_k2n.py --cfg configs/medicalmri.yaml --max-samples 10 --visualize-mode smooth --anti-aliasing --debug --output-size 256 256 --out-dir outputs/grid_fix_256

# ============================================================================
# ğŸ“Š é¢„æœŸæ€§èƒ½æå‡ (feat_dim=512 -> 256):
# - å‚æ•°é‡: 111M -> 76M (å‡å°‘31.9%)
# - æ¨ç†é€Ÿåº¦: æå‡ ~25-35%
# - æ˜¾å­˜å ç”¨: å‡å°‘ ~20-30%  
# - è´¨é‡æŒ‡æ ‡: PSNR/SSIMæŸå¤± < 5% (éœ€å®æµ‹éªŒè¯)
# ============================================================================

# ğŸš¨ å…³äºæ ¼çº¹ä¼ªå½±çš„è¯´æ˜ï¼š
# æ ¼çº¹ä¼ªå½±é€šå¸¸è¡¨æ˜ä»¥ä¸‹é—®é¢˜ä¹‹ä¸€ï¼š
# 1. ä¸Šé‡‡æ ·å±‚è®¾è®¡ä¸å½“ï¼ˆå¦‚è½¬ç½®å·ç§¯çš„æ­¥é•¿å’Œæ ¸å¤§å°ä¸åŒ¹é…ï¼‰
# 2. è®­ç»ƒä¸å……åˆ†ï¼Œæ¨¡å‹å°šæœªå­¦ä¼šç”Ÿæˆå¹³æ»‘çº¹ç†
# 3. ç¼ºå°‘æ„ŸçŸ¥æŸå¤±æˆ–å¯¹æŠ—æŸå¤±æ¥çº¦æŸç”Ÿæˆè´¨é‡
# 4. ç½‘ç»œæ¶æ„ä¸­çš„ç‰¹å¾èåˆæ–¹å¼äº§ç”Ÿä¼ªå½±
# 5. è¾“å‡ºå°ºå¯¸ä¸è®­ç»ƒå°ºå¯¸ä¸åŒ¹é…å¯¼è‡´çš„æ’å€¼ä¼ªå½±
# 
# è§£å†³æ–¹æ¡ˆï¼š
# - ä½¿ç”¨ --output-size 256 256 ç¡®ä¿è¾“å‡ºä¸è®­ç»ƒæ•°æ®å°ºå¯¸ä¸€è‡´
# - ä½¿ç”¨ --anti-aliasing æ ‡å¿—åº”ç”¨é«˜æ–¯å¹³æ»‘
# - ä½¿ç”¨ --visualize-mode smooth è¿›è¡Œå¹³æ»‘å¯è§†åŒ–  
# - æ£€æŸ¥æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å‡½æ•°è®¾è®¡
# - è€ƒè™‘ä½¿ç”¨åŒçº¿æ€§ä¸Šé‡‡æ ·+å·ç§¯æ›¿ä»£è½¬ç½®å·ç§¯
# ============================================================================
